{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "df = pd.read_excel(r'/workspaces/ofsted-ilacs-scrape-tool/Textual analysis/ofsted_childrens_services_overview.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Inspection Barnsley local authority children s...\n",
      "1      Inspection Bath North East Somerset local auth...\n",
      "2      Inspection Bedford Borough local authority chi...\n",
      "3      Inspection Birmingham City cil local authority...\n",
      "4      Inspection Blackburn Darwen local authority ch...\n",
      "                             ...                        \n",
      "146    Inspection Wiltshire Council local authority c...\n",
      "147    Inspection Wirral local authority children ser...\n",
      "148    Inspection Wokingham Borough Council local aut...\n",
      "149    Inspection City Wolverhampton local authority ...\n",
      "150    Inspection Worcestershire local authority chil...\n",
      "Name: full text lem, Length: 151, dtype: object\n"
     ]
    }
   ],
   "source": [
    "regexp = RegexpTokenizer('\\w+')\n",
    "\n",
    "df['tokens'] = df['full text'].apply(regexp.tokenize)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#extend stopwords as needed\n",
    "my_stopwords = []\n",
    "stopwords.extend(my_stopwords)\n",
    "\n",
    "# remove stopwords\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "\n",
    "# remove all words 2 letters and under\n",
    "df['word string'] = df['tokens'].apply(lambda x: ' '.join(item for item in x if len(item)>2))\n",
    "\n",
    "words = ' '.join(word for word in df['word string'])\n",
    "\n",
    "tokenized_words = nltk.tokenize.word_tokenize(words)\n",
    "\n",
    "words_fdist = FreqDist(tokenized_words)\n",
    "\n",
    "df['full text fdist'] = df['tokens'].apply(lambda x: ' '.join([item for item in x if words_fdist[item] >= 3]))\n",
    "\n",
    "wordnet_lem = WordNetLemmatizer()\n",
    "df['full text lem'] = df['full text fdist'].apply(wordnet_lem.lemmatize)\n",
    "print(df['full text lem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      1.0\n",
       "2      1.0\n",
       "3      1.0\n",
       "4      1.0\n",
       "      ... \n",
       "146    1.0\n",
       "147    1.0\n",
       "148    1.0\n",
       "149    1.0\n",
       "150    1.0\n",
       "Name: polarity, Length: 151, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "\n",
    "    sentiment = scores['compound']\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "df['polarity'] = df['full text lem'].apply(get_sentiment)\n",
    "\n",
    "df['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.039, 'neu': 0.664, 'pos': 0.297, 'compound': 1.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['polarity'].iloc[0]['compound']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
